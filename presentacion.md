---
marp: true
theme: default
paginate: true
---

# ğŸŒŒ Modelos de DifusiÃ³n y Stable Diffusion
### CÃ³mo las mÃ¡quinas generan imÃ¡genes a partir de texto

<!--
Notas:
Bienvenidos a esta pÃ­ldora formativa.  
Hoy vamos a descubrir quÃ© son los modelos de difusiÃ³n, cÃ³mo funcionan en teorÃ­a, cÃ³mo se aplican a imÃ¡genes y quÃ© hace que Stable Diffusion sea tan especial.  
El objetivo es que todos entendamos el concepto desde cero, pero tambiÃ©n que veamos su parte tÃ©cnica y prÃ¡ctica.
-->

---

# ğŸ¤– Â¿QuÃ© son los modelos de difusiÃ³n?
- Modelos **generativos** de Deep Learning.  
- DiseÃ±ados para **crear nuevos datos** a partir de lo aprendido.  
- En el caso de Stable Diffusion â†’ **imÃ¡genes**.  

<!--
Notas:
Stable Diffusion pertenece a la familia de modelos de difusiÃ³n.  
Estos modelos son generativos, es decir, crean nuevos datos que se parecen a los ejemplos que vieron durante su entrenamiento.  
En nuestro caso, hablamos de imÃ¡genes. Pero la idea general tambiÃ©n se aplica a audio, vÃ­deo o incluso molÃ©culas.
-->

---

# ğŸ”„ El proceso de difusiÃ³n
- **Dos fases principales**:
  1. DifusiÃ³n directa â†’ aÃ±adir ruido progresivo.
  2. DifusiÃ³n inversa â†’ eliminar el ruido paso a paso.  

---

# ğŸŒ«ï¸ Fase 1: DifusiÃ³n directa
Imagen â†’ Ruido paso a paso â†’ Imagen irreconocible

<!--
Notas:
En la fase directa, cogemos una imagen del dataset y le vamos aÃ±adiendo ruido gaussiano.  
Al principio la imagen todavÃ­a se reconoce, pero a medida que aÃ±adimos mÃ¡s pasos se va desfigurando hasta volverse puro ruido.  
Este proceso se puede hacer matemÃ¡ticamente sin problema.
-->

---

# âœ¨ Fase 2: DifusiÃ³n inversa
Ruido â†’ EliminaciÃ³n progresiva â†’ Imagen reconstruida

<!--
Notas:
La parte difÃ­cil llega cuando intentamos revertir el proceso.  
Â¿CÃ³mo sabemos cuÃ¡nto ruido quitar en cada paso?  
Â¿CuÃ¡ntos pasos necesitamos?  
Para resolverlo, se entrena una red neuronal especializada que predice el nivel de ruido en cada iteraciÃ³n.
-->

---

# ğŸ§  La red U-Net
- Arquitectura clave en la fase inversa.  
- Aprende a predecir **quÃ© ruido quitar** en cada paso.  
- Entrada: imagen ruidosa.  
- Salida: estimaciÃ³n de ruido.  

<!--
Notas:
AquÃ­ aparece la red U-Net.  
Es una arquitectura muy usada en visiÃ³n por computadora porque combina una parte de contracciÃ³n (extraer caracterÃ­sticas) con una de expansiÃ³n (reconstrucciÃ³n).  
En Stable Diffusion, su papel es predecir el ruido que hay que eliminar.  
Gracias a U-Net podemos partir de una nube de ruido puro y acabar con una imagen coherente.
-->

---

# ğŸ¨ Condicionamientos
- El modelo necesita **control creativo**.  
- Tipos de condicionamiento:
  - Texto (prompt).  
  - Mapas de profundidad.  
  - MÃ¡scaras de ediciÃ³n.  
  - ImÃ¡genes de referencia.  

<!--
Notas:
El modelo por sÃ­ mismo solo denoisea, pero no sabe quÃ© imagen generar.  
Para guiarlo usamos condicionamientos.  
El mÃ¡s famoso: los prompts de texto. Escribir â€œun perro astronauta en estilo Van Goghâ€ hace que la U-Net reconstruya el ruido en esa direcciÃ³n.  
Pero tambiÃ©n se pueden usar mapas de profundidad, mÃ¡scaras para editar partes de la imagen, o incluso imÃ¡genes de referencia.  
Esto es lo que hace que podamos tener un control casi artÃ­stico sobre el proceso.
-->

---

# âš¡ El problema de los costes
- Modelos clÃ¡sicos trabajan en **espacio de pÃ­xeles**.  
- Ejemplo: imagen 512Ã—512 RGB = ~800.000 valores.  
- AÃ±adir/quitar ruido sobre todos esos valores â†’ **muy lento y costoso**.  

<!--
Notas:
Los modelos de difusiÃ³n originales operaban directamente sobre los pÃ­xeles de la imagen.  
El problema es que una sola imagen de 512Ã—512 pÃ­xeles y tres canales de color implica manejar casi un millÃ³n de valores.  
Y recordad que el proceso de difusiÃ³n implica cientos o miles de pasos.  
Esto hace que los cÃ¡lculos sean muy pesados y lentos.
-->

---

# ğŸ’¡ La innovaciÃ³n: espacio latente
- Stable Diffusion introduce los **Modelos de DifusiÃ³n Latente**.  
- En lugar de trabajar con pÃ­xeles, lo hace en un espacio matemÃ¡tico **comprimido**.  
- Beneficio: **eficiencia y rapidez**.  

<!--
Notas:
La soluciÃ³n de Stability AI fue operar en un espacio mÃ¡s compacto: el espacio latente.  
AquÃ­ la imagen se representa en menos dimensiones, lo que reduce drÃ¡sticamente los cÃ¡lculos.  
En lugar de trabajar con todos los pÃ­xeles, trabajamos con una versiÃ³n comprimida que conserva la esencia visual.
-->

---

# ğŸŒ€ El papel del VAE
- **Variational Autoencoder (VAE)**: red con dos partes.
  - **Encoder (E):** lleva la imagen al espacio latente.  
  - **Decoder (D):** reconstruye la imagen desde el espacio latente.  

<!--
Notas:
Para movernos entre el espacio de pÃ­xeles y el espacio latente necesitamos un VAE.  
El encoder comprime la imagen en una representaciÃ³n abstracta, y el decoder la reconstruye despuÃ©s.  
AsÃ­ todo el proceso de difusiÃ³n ocurre en el espacio latente, y solo al final recuperamos la imagen visible.
-->

---

# ğŸ”— Uniendo todo
Stable Diffusion combina:
- DifusiÃ³n en espacio latente (eficiencia).  
- U-Net (predicciÃ³n del ruido).  
- Condicionamientos (control creativo).  

<!--
Notas:
Si juntamos todas las piezas tenemos Stable Diffusion:  
1. Un modelo eficiente gracias al espacio latente.  
2. Preciso gracias a la U-Net que elimina el ruido.  
3. Creativo y versÃ¡til gracias a los condicionamientos.  
El resultado es un modelo ligero, versÃ¡til y open source.
-->

---

# ğŸš€ Stable Diffusion
- **Ligero**: puede ejecutarse en PCs con GPU comÃºn.  
- **VersÃ¡til**: soporta texto, imÃ¡genes, mÃ¡scaras.  
- **Open Source**: la comunidad crea y mejora variantes.  

<!--
Notas:
Por todo esto Stable Diffusion se volviÃ³ revolucionario en 2022.  
No solo democratizÃ³ el acceso a la IA generativa de imÃ¡genes, sino que al ser open source, miles de personas han creado sus propias variantes.  
Hoy existen modelos especializados en retratos, anime, arquitectura, medicina, etc.
-->

---

# ğŸŒ Aplicaciones y futuro
âœ… Arte y diseÃ±o  
âœ… Marketing y creatividad  
âœ… Videojuegos  
âœ… Medicina e investigaciÃ³n  
âš ï¸ Riesgos: deepfakes, copyright, sesgos  

<!--
Notas:
Las aplicaciones son enormes: desde arte digital y videojuegos, hasta investigaciÃ³n mÃ©dica.  
Pero no podemos olvidar los riesgos: la creaciÃ³n de deepfakes, problemas de copyright, y los sesgos en los datos de entrenamiento.  
El futuro apunta a modelos multimodales capaces de trabajar no solo con imÃ¡genes, sino tambiÃ©n con audio y vÃ­deo.
-->

---

# ğŸ“š ConclusiÃ³n
- Los modelos de difusiÃ³n generan imÃ¡genes **a partir de ruido**.  
- Stable Diffusion lo hace **eficiente** en espacio latente.  
- Nos da **control creativo** mediante condicionamientos.  
- Y es **open source**, impulsando una gran comunidad.  

<!--
Notas:
Cerramos con un resumen:  
1. Los modelos de difusiÃ³n son capaces de imaginar a partir de ruido.  
2. Stable Diffusion optimiza este proceso en el espacio latente.  
3. Gracias a los condicionamientos podemos guiar la creatividad.  
4. Y al ser open source, se ha convertido en un estÃ¡ndar de la comunidad.  
La idea final: lo que antes era ciencia ficciÃ³n, hoy estÃ¡ al alcance de cualquiera con un ordenador.
-->
